{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNR2FCgw9caIe9nR7XdmNgI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"XW4551BTEXvt","executionInfo":{"status":"ok","timestamp":1701269903469,"user_tz":-330,"elapsed":470,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n"]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert (\n","            self.head_dim * heads == embed_size\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","\n"],"metadata":{"id":"tl1h_8iwE3y0","executionInfo":{"status":"ok","timestamp":1701269905636,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def forward(self, values, keys, query, mask):\n","\n","        N = query.shape[0]  #  number of training examples\n","\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        values = self.values(values)\n","        keys = self.keys(keys)\n","        queries = self.queries(queries)\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","\n","        out = self.fc_out(out)\n","        return out\n","\n"],"metadata":{"id":"Y_5Q5_-RFBmJ","executionInfo":{"status":"ok","timestamp":1701269911203,"user_tz":-330,"elapsed":453,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, embed_size, ff_hidden_size):\n","        super(FeedForward, self).__init__()\n","        self.fc1 = nn.Linear(embed_size, ff_hidden_size)\n","        self.fc2 = nn.Linear(ff_hidden_size, embed_size)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n"],"metadata":{"id":"5X1IONLOFIgd","executionInfo":{"status":"ok","timestamp":1701269915804,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class GPT2Layer(nn.Module):\n","    def __init__(self, embed_size, heads, ff_hidden_size, dropout=0.1):\n","        super(GPT2Layer, self).__init__()\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","        self.attn = MultiHeadAttention(embed_size, heads)\n","        self.ff = FeedForward(embed_size, ff_hidden_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        x = x + self.dropout(self.attn(x, x, x, mask))\n","        x = self.norm1(x)\n","        x = x + self.dropout(self.ff(x))\n","        x = self.norm2(x)\n","        return x\n","\n"],"metadata":{"id":"b7n2lIDAFL17","executionInfo":{"status":"ok","timestamp":1701269919677,"user_tz":-330,"elapsed":2,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class GPT2(nn.Module):\n","    def __init__(self, vocab_size, embed_size, heads, ff_hidden_size, num_layers, dropout=0.5):\n","        super(GPT2, self).__init__()\n","        self.embed_size = embed_size\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.layers = nn.ModuleList(\n","            [\n","                GPT2Layer(embed_size, heads, ff_hidden_size, dropout)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.fc = nn.Linear(embed_size, vocab_size)\n","\n","    def forward(self, x, mask):\n","        x = self.embedding(x)\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        x = self.fc(x)\n","        return x"],"metadata":{"id":"Lcdqu0T5FPqC","executionInfo":{"status":"ok","timestamp":1701269925220,"user_tz":-330,"elapsed":1211,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6rXfShcAFRgW"},"execution_count":null,"outputs":[]}]}