{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSGzKsECKXIfusmOkPxQ7l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7OU8NN0h_pr","executionInfo":{"status":"ok","timestamp":1701268973273,"user_tz":-330,"elapsed":5686,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}},"outputId":"1ab9bdfd-cf15-4c2e-ac81-fee69f6391e5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","from einops.layers.torch import Rearrange"],"metadata":{"id":"K8LEZkAvg89_","executionInfo":{"status":"ok","timestamp":1701268991067,"user_tz":-330,"elapsed":2812,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class RotaryEmbedding(nn.Module):\n","    def __init__(self, embed_size):\n","        super(RotaryEmbedding, self).__init__()\n","        self.embed_size = embed_size\n","        self.freqs = 2 ** torch.linspace(0, embed_size // 2 - 1, embed_size // 2)\n","        self.scale = nn.Parameter(torch.ones(embed_size // 2))\n","\n","    def forward(self, x):\n","        x = x + 0.5  # Centering the inputs around 0\n","        x = x.unsqueeze(-2) * self.freqs.unsqueeze(0)\n","        x = torch.cat([torch.sin(x), torch.cos(x)], dim=-1)\n","        x = x * self.scale.unsqueeze(0)\n","        return x"],"metadata":{"id":"UkMR2kyshtxO","executionInfo":{"status":"ok","timestamp":1701269028514,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class GPT2WithRotary(nn.Module):\n","    def __init__(self, vocab_size, embed_size=768, heads=12, num_layers=12, ff_hidden_size=3072, dropout=0.1):\n","        super(GPT2WithRotary, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rotary_embedding = RotaryEmbedding(embed_size)\n","        self.blocks = nn.Sequential(*[GPT2Block(embed_size, heads, ff_hidden_size, dropout) for _ in range(num_layers)])\n","        self.fc = nn.Linear(embed_size, vocab_size)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x + self.rotary_embedding(torch.arange(x.size(1), device=x.device).float())\n","        x = self.blocks(x)\n","        x = self.fc(x)\n","        return x"],"metadata":{"id":"3NpfzsCMiTdC","executionInfo":{"status":"ok","timestamp":1701269041855,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saumya Chaudhary 4-Year B.Tech. Electronics Engineering","userId":"11792631033894863156"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m1nw5NUqiWm6"},"execution_count":null,"outputs":[]}]}